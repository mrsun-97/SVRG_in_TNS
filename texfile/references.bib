@book{adams1995hitchhiker,
  title={The Hitchhiker's Guide to the Galaxy},
  author={Adams, D.},
  isbn={9781417642595},
  url={http://books.google.com/books?id=W-xMPgAACAAJ},
  year={1995},
  publisher={San Val}
}


@misc{noauthor_sgd_nodate,
	title = {{如何让SGD更稳更快}？},
	url = {https://zhuanlan.zhihu.com/p/162977120},
	abstract = {我们在训练机器学习模型的时候一般会使用随机梯度下降(SGD)。但SGD有个显著的缺点，那就是到了后期loss会起起伏伏，令我们炼丹师头秃…},
	language = {zh},
	urldate = {2020-10-14},
	journal = {知乎专栏},
	file = {}
}

@article{keskarImprovingGeneralizationPerformance2017,
  title = {Improving {{Generalization Performance}} by {{Switching}} from {{Adam}} to {{SGD}}},
  author = {Keskar, Nitish Shirish and Socher, Richard},
  year = {2017},
  month = dec,
  abstract = {Despite superior training outcomes, adaptive optimization methods such as Adam, Adagrad or RMSprop have been found to generalize poorly compared to Stochastic gradient descent (SGD). These methods tend to perform well in the initial portion of training but are outperformed by SGD at later stages of training. We investigate a hybrid strategy that begins training with an adaptive method and switches to SGD when appropriate. Concretely, we propose SWATS, a simple strategy which switches from Adam to SGD when a triggering condition is satisfied. The condition we propose relates to the projection of Adam steps on the gradient subspace. By design, the monitoring process for this condition adds very little overhead and does not increase the number of hyperparameters in the optimizer. We report experiments on several standard benchmarks such as: ResNet, SENet, DenseNet and PyramidNet for the CIFAR-10 and CIFAR-100 data sets, ResNet on the tiny-ImageNet data set and language modeling with recurrent networks on the PTB and WT2 data sets. The results show that our strategy is capable of closing the generalization gap between SGD and Adam on a majority of the tasks.},
  archivePrefix = {arXiv},
  eprint = {1712.07628},
  eprinttype = {arxiv},
  file = {/home/parry/Zotero/storage/E2BNPL6A/Keskar 和 Socher - 2017 - Improving Generalization Performance by Switching .pdf;/home/parry/Zotero/storage/C72RV59Z/1712.html},
  journal = {arXiv:1712.07628 [cs, math]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}



@article{reddiConvergenceAdam2019,
  title = {On the {{Convergence}} of {{Adam}} and {{Beyond}}},
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  year = {2019},
  month = apr,
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSProp, Adam, Adadelta, Nadam are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where Adam does not converge to the optimal solution, and describe the precise problems with the previous analysis of Adam algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with `long-term memory' of past gradients, and propose new variants of the Adam algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  archivePrefix = {arXiv},
  eprint = {1904.09237},
  eprinttype = {arxiv},
  file = {/home/parry/Zotero/storage/A6CBWELZ/Reddi 等。 - 2019 - On the Convergence of Adam and Beyond.pdf;/home/parry/Zotero/storage/I5DTUJSQ/forum.html;/home/parry/Zotero/storage/MX7XM3QG/1904.html},
  journal = {arXiv:1904.09237 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}



@article{johnsonAcceleratingStochasticGradient,
  title = {Accelerating {{Stochastic Gradient Descent}} Using {{Predictive Variance Reduction}}},
  author = {Johnson, Rie and Zhang, Tong},
  pages = {9},
  abstract = {Stochastic gradient descent is popular for large scale optimization but has slow convergence asymptotically due to the inherent variance. To remedy this problem, we introduce an explicit variance reduction method for stochastic gradient descent which we call stochastic variance reduced gradient (SVRG). For smooth and strongly convex functions, we prove that this method enjoys the same fast convergence rate as those of stochastic dual coordinate ascent (SDCA) and Stochastic Average Gradient (SAG). However, our analysis is significantly simpler and more intuitive. Moreover, unlike SDCA or SAG, our method does not require the storage of gradients, and thus is more easily applicable to complex problems such as some structured prediction problems and neural network learning.},
  file = {/home/parry/Zotero/storage/GBQTA37I/Johnson and Zhang - Accelerating Stochastic Gradient Descent using Pre.pdf},
  language = {en}
}


