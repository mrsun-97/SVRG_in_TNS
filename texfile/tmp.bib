
@article{rouxStochasticGradientMethod,
  title = {A {{Stochastic Gradient Method}} with an {{Exponential Convergence}} \_{{Rate}} for {{Finite Training Sets}}},
  author = {Roux, Nicolas L and Schmidt, Mark and Bach, Francis R},
  pages = {9},
  abstract = {We propose a new stochastic gradient method for optimizing the sum of a finite set of smooth functions, where the sum is strongly convex. While standard stochastic gradient methods converge at sublinear rates for this problem, the proposed method incorporates a memory of previous gradient values in order to achieve a linear convergence rate. In a machine learning context, numerical experiments indicate that the new algorithm can dramatically outperform standard algorithms, both in terms of optimizing the training error and reducing the test error quickly.},
  file = {/home/parry/Zotero/storage/5SM7Z4Y3/Roux et al. - A Stochastic Gradient Method with an Exponential C.pdf},
  language = {en}
}


